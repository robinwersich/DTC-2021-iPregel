---
title: "StackOverflow Node Importance Measure Analysis"
output: html_notebook
---

## SetUp
```{r}
library(ggplot2)
graph.color <- "#6fa8dc"
border.color <- "grey30"
```


## 0. Dataset description

Similar to the Twitter network, the stackoverflow network contained an edge from a user A to a user B if A answered or commented a question (or answer) of user B, labelled with a timestamp which we removed. Again resulting duplicates, as well as self-references, were removed. Unlike on Twitter, on stackoverflow not users that are interacted with (high in-degree) are important, but users that interact (high out-degree), because they are the people knowledgeable enough to answer questions. Consistently, people who can answer questions of people with great knowledge must have great knowledge themselves (similar to how people mentioned by influential people on Twitter also gain influence). For this reason, we decided to reverse the edges in the stackoverflow graph, so people with high knowledge also have a high in-degree and will thus be considered important by algorithms like PageRank.

## 1. Load Node Importance Measure and Meta Data

```{r}
pr = read.csv("../results/networkit_pagerank_stackoverflow.txt", sep="\t", header = FALSE)
colnames(pr) <- c("ID", "PageRank")

bc = read.csv("../results/networkit_betweenness_centrality_stackoverflow_reversed.txt", sep="\t", header = FALSE) 
colnames(bc) <- c("ID", "BetweennessCentrality")
#bc <- bc[bc$BetweennessCentrality != 0, ]

cc = read.csv("../results/networkit_closeness_centrality_stackoverflow_reversed.txt", sep="\t", header = FALSE)
colnames(cc) <- c("ID", "ClosenessCentrality")

new_measures <- list("PageRank"=pr,"BetweennessCentrality"=bc,"ClosenessCentrality"=cc)

# transform ids to be 1-indexed to be able to work easily with them in R
bc$ID <- bc$ID + 1
cc$ID <- cc$ID + 1
pr$ID <- pr$ID + 1

```

## 2. Feasibility of PageRank, Betweenness Centrality and Closeness Centrality as Node Importance Measures

To evaluate feasibility, we assess whether each node got assigned a meaningful score and if the resulting distribution of scores serves well for distinguishing nodes. We do this via visual inspection of the histograms of the resulting distributions per measure. As some measures have a huge spread of values (e.g. Betweenness Centrality) we plot a histogram with outliers, a histogram of only the interquartile range and additionally histograms of the outlier distribution and inspect all of them.



### Histograms including outliers

```{r}
bins <- list(PageRank=40, BetweennessCentrality=40, ClosenessCentrality=40)

for (measure in names(new_measures)) {
  p <- ggplot(new_measures[[measure]], aes_string(x=measure)) + geom_histogram(bins=bins[[measure]], fill=graph.color, color=border.color) + labs(title=paste(measure, "Distribution"), y="Count")
  print(p)
}
```

#### Debugging occurrences

```{r}
pagerank.occr <- table(unlist(new_measures[["PageRank"]][["PageRank"]]))
betweenness.centrality.occr <- table(unlist(new_measures[["BetweennessCentrality"]][["BetweennessCentrality"]]))
closeness.centrality.occr <- table(unlist(new_measures[["ClosenessCentrality"]][["ClosenessCentrality"]]))
```

```{r}
summary(new_measures[["PageRank"]])
summary(new_measures[["BetweennessCentrality"]])
summary(new_measures[["ClosenessCentrality"]])
```


## Print Outlier Distributions

```{r}
library(ggplot2)
outliers <- list()
bins <- list(PageRank=10000, BetweennessCentrality=40, ClosenessCentrality=40)


for (measure in names(new_measures)) {
  cur.measure.stats <- boxplot.stats(new_measures[[measure]][[measure]])
  cur.measure.median <- cur.measure.stats$stats[3]
  cur.measure.outliers <- cur.measure.stats$out
  
  outliers[[measure]] <- list()
  outliers[[measure]][["total"]] <- cur.measure.outliers
  outliers[[measure]][["upper"]] <- cur.measure.outliers[cur.measure.outliers > cur.measure.median]
  outliers[[measure]][["lower"]] <- cur.measure.outliers[cur.measure.outliers < cur.measure.median]
}

for (measure in names(outliers)) {
  p <- ggplot() + aes_string(outliers[[measure]][["upper"]]) + geom_histogram(bins=bins[[measure]], fill=graph.color, color=border.color) + labs(title=paste(measure,"Interquartile Range Upper Outlier Distribution"), x=measure, y="Count")
  print(p)
  p <- ggplot() + aes_string(outliers[[measure]][["lower"]]) + geom_histogram(bins=bins[[measure]], fill=graph.color, color=border.color) + labs(title=paste(measure, "Interquartile Range Lower Outlier Distribution"), x=measure, y="Count")
  print(p)
}
```

## Print Interquartile Range Distribution
```{r}
for (measure in names(outliers)) {
  cur.measure <- new_measures[[measure]]
  cur.outliers <- outliers[[measure]][["total"]]
  p <- ggplot(cur.measure[!(cur.measure[[measure]] %in%  cur.outliers),]) + aes_string(x=measure) + geom_histogram(bins=bins[[measure]], fill=graph.color, color=border.color)
  p <- p + labs(title=paste(measure, "Interquartile Range Distribution"), x=measure, y="Count")
 print(p)
}
```

## 4. Network properties

We investigated the number of people that interacted only once, with only one outgoing edge, to find out what causes the huge amount of nodes being assigned the minimum value for each measure.

```{r}
network = read.csv("../data_prepared/stackoverflow.txt", sep=" ", header = FALSE)
colnames(network) <- c("Source", "Target")
source.with.one.outgoing_edge <- which(!(duplicated(network$Source)|duplicated(network$Source, fromLast=TRUE)))
# make sure source nodes are not target nodes
source.with.one.outgoing_edge <- source.with.one.outgoing_edge[! source.with.one.outgoing_edge %in% network$Target]
```
